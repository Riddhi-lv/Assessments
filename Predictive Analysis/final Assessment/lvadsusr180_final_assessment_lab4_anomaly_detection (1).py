# -*- coding: utf-8 -*-
"""lvadsusr180 final assessment lab4_anomaly_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BVFpFzrtmAXIrj7EpvhgQtckbsYnCrrK
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest

from sklearn.preprocessing import StandardScaler

df=pd.read_csv(r'https://raw.githubusercontent.com/Deepsphere-AI/LVA-Batch5-Assessment/main/anomaly_train.csv')

df.shape

df.head()

df.duplicated().sum()

df.drop('TransactionID',axis=1,inplace=True)

df['Location'].value_counts()

df['Type'].value_counts()

df=pd.get_dummies(data=df,columns=['Location','Type'],dtype='int',drop_first=True)

for i in df.columns.tolist():
  sns.boxplot(data=df,x=i)
  plt.show()

iforest=IsolationForest(contamination=0.02)

iforest.fit(df[['Time','Amount']])

df['anomaly_score']=iforest.decision_function(df[['Time','Amount']])

df['anomaly'] = iforest.predict(df[['Time','Amount']])

df.head()

ss=StandardScaler()
cols=['Amount','Time']
scaled_data=ss.fit_transform(df[cols])
df.drop(cols,axis=1)
df1=pd.DataFrame(columns=cols,data=scaled_data)
data=pd.concat([df,df1],axis=1)

anomalies=df[df['anomaly_score']<0]

print(anomalies)

plt.scatter(df["Time"],df["Amount"] ,color='g', label="Normal")
plt.scatter(anomalies["Time"],anomalies["Amount"] , color="r", label="Anomaly")
plt.xlabel("Amount")
plt.ylabel("Time")
plt.legend()
plt.show()





